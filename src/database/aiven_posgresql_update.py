import pandas as pd
import psycopg2
from psycopg2 import sql
from src.logging import logging
from pandas.io.sql import get_schema
from src.constants import POSTGRES_DB_NAME, POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_TABLE_NAME
import json

logger = logging()

class PostgresDataBaseUpdate:
    def __init__(self):
        logger.info('PostgresDataBaseUpdate instance created.')

    def update_to_postgres_database(self, df: pd.DataFrame = None, table_name: str = POSTGRES_TABLE_NAME) -> None:
        """
        Updates the PostgreSQL database with the given DataFrame.
        Drops the existing table, creates a new one based on the DataFrame's schema,
        and inserts all data in a single batch transaction.
        Fully handles column names with spaces or special characters.
        """
        # Validate credentials
        required_vars = [POSTGRES_DB_NAME, POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT]
        if not all(required_vars):
            error_msg = "PostgreSQL credentials (DB_NAME, USER, PASSWORD, HOST, PORT) are not set."
            logger.error(error_msg)
            raise ValueError(error_msg)

        # Validate DataFrame
        if df is None or df.empty:
            error_msg = "DataFrame is None or empty"
            logger.error(error_msg)
            raise ValueError(error_msg)

        conn = None
        try:
            logger.info(f"Connecting to PostgreSQL database: {POSTGRES_DB_NAME}")
            conn = psycopg2.connect(
                dbname=POSTGRES_DB_NAME,
                user=POSTGRES_USER,
                password=POSTGRES_PASSWORD,
                host=POSTGRES_HOST,
                port=POSTGRES_PORT
            )
            cur = conn.cursor()

            # 1. Drop the existing table
            drop_table_sql = sql.SQL("DROP TABLE IF EXISTS {}").format(sql.Identifier(table_name))
            cur.execute(drop_table_sql)

            # 2. Create table schema (leave all quotes as generated by pandas)
            create_table_sql = get_schema(df, table_name, con=conn)
            cur.execute(create_table_sql)

            # 3. Prepare batched INSERT statements with all columns double-quoted
            columns = list(df.columns)
            quoted_columns = [sql.Identifier(col) for col in columns]
            insert_query = sql.SQL("INSERT INTO {} ({}) VALUES ({})").format(
                sql.Identifier(table_name),
                sql.SQL(', ').join(quoted_columns),
                sql.SQL(', ').join(sql.Placeholder() * len(columns))
            )

            # 4. Convert dicts to JSON strings before insert
            def adapt_dicts(val):
                if isinstance(val, dict):
                    return json.dumps(val)
                return val

            values = [
                tuple(adapt_dicts(None if pd.isna(val) else val) for val in row)
                for _, row in df.iterrows()
            ]
            cur.executemany(insert_query, values)

            conn.commit()
            logger.info(f"Successfully updated PostgreSQL table: {table_name}")

        except Exception as e:
            logger.error(f"An error occurred while updating PostgreSQL database: {e}")
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                conn.close()